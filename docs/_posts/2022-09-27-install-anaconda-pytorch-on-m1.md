---
layout: post
title:  "Apple Silicon(M1) ì— Anaconda & PyTorch ì„¤ì¹˜í•˜ê¸° + ì„±ëŠ¥ ì¸¡ì •"
date:   2022-09-27
categories: ETC
---
# Apple Siliconì— PyTorch ì„¤ì¹˜í•˜ê¸°

ì•„ë§ˆ 2020ë…„ì´ì—ˆë‚˜â€¦ ì• í”Œ ì‹¤ë¦¬ì½˜ ì•„í‚¤í…ì³ì˜ Macbookì´ ì¶œì‹œë˜ì—ˆë‹¤.

ë‚˜ëŠ” í˜„ì¬ M1 macbook proë¥¼ ì“°ê³ ìˆëŠ”ë°, ì´ì „ì—ëŠ” anacondaë„ ì•ˆë˜ì„œ mini-forgeë¼ëŠ” ê±¸ ì‚¬ìš©í–ˆì—ˆë‹¤.

ê·¸ë¦¬ê³  ê·¸ mini-forgeì—ëŠ” ë§ë„ ì•ˆë˜ëŠ” ë°˜ìª½ì§œë¦¬ pytorchë¥¼ ì„¤ì¹˜í•´ì„œ ì‚¬ìš©í–ˆì—ˆë‹¤.

ì´ pytorchê°€ êµ¬ë™ì´ ì•ˆë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ì–´ì°Œì–´ì°Œí•´ì„œ ì„¤ì¹˜í•´ì„œ í•˜ë©´ ë­”ê°€ ë˜ê¸´í–ˆì—ˆëŠ”ë° GPU ê°€ì†ë„ ì•ˆë˜ê³  ì—¬ëŸ¬ ì œì•½ì‚¬í•­ì´ ìˆì—ˆë‹¤.
(ë¬¼ë¡  ê°œë°œì€ ì„œë²„ê°€ ìˆê¸° ë•Œë¬¸ì— ë”±íˆ ìƒê´€ì€ ì—†ì—ˆëŠ”ë°â€¦ ê°€ë²¼ìš´ ê²ƒì´ë¼ë„ ì˜ ëŒë¦¬ê³  ì‹¶ê¸´ í•˜ë‹ˆê¹Œ?!)

í•˜ì§€ë§Œ! ë“œë””ì–´ 

> ğŸ’¡ **Apple Silicon(M1) ì„ ìœ„í•œ Anaconda ì™€ PyTorchê°€ ì •ì‹ ì¶œì‹œë˜ì—ˆë‹¤!**

- Pytorch ê´€ë ¨ ê¸°ì‚¬ : [Introducing Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/?fbclid=IwAR25rWBO7pCnLzuOLNb2rRjQLP_oOgLZmkJUg2wvBdYqzL72S5nppjg9Rvc)

ì•„ì§ Preview ë²„ì „ì´ë¼ëŠ”ê±°ë‹ˆê¹Œ ì •ì‹(?)ì€ ì•„ë‹Œê±¸ë¡œâ€¦

## 1. Anaconda ì„¤ì¹˜

Anaconda ì„¤ì¹˜ëŠ” ë§¤ìš° ì‰½ë‹¤. Anaconda ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œ pkg íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•˜ì—¬ ê·¸ëƒ¥ ë”ë¸”í´ë¦­í•˜ê³  ì„¤ì¹˜í•˜ë©´ ëœë‹¤.

1. **Anaconda download page ë¥¼ ë“¤ì–´ê°„ë‹¤.** : [Anaconda | Anaconda Distribution](https://www.anaconda.com/products/distribution)  

2. **ì‚¬ê³¼ ì•„ì´ì½˜ì„ í´ë¦­í•œë‹¤. (ë§¨ ì•„ë˜ë¡œ ì´ë™ë¨.)**
![](https://velog.velcdn.com/images/bolero2/post/ee7a8a87-acfe-4614-8d50-5f070c35627f/image.png)

3. **ë§¨ ì•„ë˜ì—ì„œ 64-Bit(M1) Graphical Installer (428MB) ë¥¼ í´ë¦­í•œë‹¤.**
![](https://velog.velcdn.com/images/bolero2/post/0ef75259-038b-4a76-ae3e-51b78c344a8f/image.png)

4. **ê·¸ëŸ¬ë©´ ì•„ë˜ì²˜ëŸ¼ pkg íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ëœë‹¤. (arm64=apple silicon ì•„í‚¤í…ì³ì„.)**
![](https://velog.velcdn.com/images/bolero2/post/4dcbb047-29ca-4bc6-b7df-39c90a463d99/image.png)

pkg íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ë©´, ì•„ë˜ì²˜ëŸ¼ ë­”ê°€ ê²½ê³ ë„ ëœ¨ê³  ê³„ì•½ì„œ ë™ì˜ë„ í•˜ê³ , ë””ìŠ¤í¬ ì§€ì •ë„ í•´ì¤˜ì•¼ í•œë‹¤.

|install-1|install-2|install-3|install-4|
|:---:|:---:|:---:|:---:|
|![](https://velog.velcdn.com/images/bolero2/post/c5a7da7a-26f5-428e-b020-42fdcf8481a3/image.png)|![](https://velog.velcdn.com/images/bolero2/post/6530967f-749b-4628-93a7-964aae4eed79/image.png)|![](https://velog.velcdn.com/images/bolero2/post/5b9e3851-29f4-45b7-8b9a-5ea2d2c76109/image.png)|![](https://velog.velcdn.com/images/bolero2/post/3a568b29-7b45-4645-9e60-f37c5ba19c93/image.png)|

ê·¸ë¦¬ê³ ëŠ” ì„¤ì¹˜í•˜ë©´ ëœë‹¤â€¦

- ì„¸íŒ…ë„ ìë™ìœ¼ë¡œ í•´ì¤˜ì„œ ë§¤~ìš° í¸í•˜ë‹¤.
![](https://velog.velcdn.com/images/bolero2/post/f6fd127e-f3a8-48b4-9ce3-400b74ad9b25/image.png)

> Anaconda ê°€ ì˜ ëœ¨ëŠ”ì§€ í™•ì¸í•˜ì.
> matplotlib íŒ¨í‚¤ì§€ê°€ ì˜ import ë˜ëŠ” ê±¸ ë³´ë‹ˆ Anacondaê°€ í‹€ë¦¼ì—†ë‹¤â€¦

- conda init ì„¸íŒ…ì€ `vi ~/.zshrc` ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
![](https://velog.velcdn.com/images/bolero2/post/fcd3c298-dfe6-48b6-b3e4-9c3a1c5624e2/image.png)

> /Users/{username}/opt/anaconda3 ì— ì‹¤ì œ íŒŒì¼ë“¤ì´ ë“¤ì–´ìˆë‹¤.
> ì‚­ì œí•˜ê³  ì‹¶ìœ¼ë©´ í•´ë‹¹ í´ë”ë¥¼ ì‚­ì œí•´ì£¼ì.

## 2. PyTorch ì„¤ì¹˜

Anaconda m1 ë²„ì „ì„ ì˜ ì„¤ì¹˜í–ˆìœ¼ë‹ˆ, PyTorchë¥¼ ì„¤ì¹˜í•´ì£¼ì.

PyTorch ì„¤ì¹˜ëŠ” ì •ë§ ë§¤ìš° ì‰½ë‹¤. 

ìš°ì„ , [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/) ë¡œ ê°€ì„œ ì„¤ì¹˜ ì»¤ë§¨ë“œë¥¼ ì•Œì•„ì•¼ í•œë‹¤.
![](https://velog.velcdn.com/images/bolero2/post/6003e2bf-22f6-4125-9494-41021b0e5bf7/image.png)

(ì•„ê¹Œ ê³µì‹ ì¹¼ëŸ¼ì—ì„œ ë³´ì•˜ë“¯ì´, Nightly ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜í•´ì•¼ í•œë‹¤.)

`pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu `

ë¡œ ì„¤ì¹˜í•˜ë©´ ëœë‹¤. ë°”ë¡œ ì„¤ì¹˜í•´ë³´ì.
![](https://velog.velcdn.com/images/bolero2/post/b13e16ec-d7de-4ad3-9ed4-733aac871746/image.png)

PyTorch ì„¤ì¹˜ë„ ëë‚¬ë‹¤.

## 3. PyTorch in M1 - GPU Acceleration

ì¼ë°˜ì ì¸ Nvidia gpuë¥¼ ì‚¬ìš©í•œë‹¤ë©´ GPU ê°€ì†í™”ë¥¼ ìœ„í•´ ì–´ë–»ê²Œ í•´ì•¼ í–ˆì„ê¹Œ.

```python
import torch

device = torch.device('cuda:0')
print(f" - device : {device}")
sample = torch.Tensor([[10, 20, 30], [30, 20, 10]])
print(f" - cpu tensor : ")
print(sample)
sample = sample.to(device)
print(f" - gpu tensor : ")
print(sample)
```

ì´ë ‡ê²Œ í•˜ë©´ gpu ê°€ì†ì´ ê°€ëŠ¥í–ˆë‹¤. (cuda:0ë²ˆ gpu deviceì— tensor í• ë‹¹)
![](https://velog.velcdn.com/images/bolero2/post/676b9471-2900-4f88-b2dd-2d399627ce8c/image.png)

ì´ë ‡ê²Œ, gpu tensor ì¶œë ¥ ë¶€ë¶„ì˜ `device` ìª½ì— **`â€˜cuda:0â€™`**ì´ ì˜ ë“¤ì–´ê°„ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

ê·¸ë ‡ë‹¤ë©´ M1ì€ ì–´ë””ì— í• ë‹¹í•´ì¤˜ì•¼ í• ê¹Œ?

M1 ì˜ ê²½ìš°ì—ëŠ”, â€˜cudaâ€™ ë§ê³  **â€˜mpsâ€™**ì— í• ë‹¹í•´ì•¼ í•œë‹¤.

```python
import torch

device = torch.device('mps')
print(f" - device : {device}")
sample = torch.Tensor([[10, 20, 30], [30, 20, 10]])
print(f" - cpu tensor : ")
print(sample)
sample = sample.to(device)
print(f" - gpu tensor : ")
print(sample)
```

í•´ë‹¹ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´
![](https://velog.velcdn.com/images/bolero2/post/7348052f-5f0b-478b-9fa3-b24dd9c9bbdd/image.png)

`device=` ìª½ì— `â€˜cuda:0â€™` ì´ ì•„ë‹Œ **`â€˜mpsâ€™`** ê°€ ë“¤ì–´ê°€ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

## 4. Speed Comparison

ì†ë„ ë¹„êµë¥¼ í•´ë³´ì.

ë‹¹ì—°íˆ nvidia gpuë³´ë‹¤ëŠ” ëŠë¦´ ê²ƒìœ¼ë¡œ ì˜ˆìƒì´ ë˜ê¸´ í•˜ì§€ë§Œâ€¦ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚ ê¹Œ?

ë¹„êµ ëŒ€ìƒì€ ì´ 4ì¢…ë¥˜ì´ë‹¤.

| Machine type | Name |
|:---|:---|
| Server GPU | Nvidia RTX A6000 |
| M1 GPU | M1 GPU, 8 Core |
| Server CPU | Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz * 40 Core |
| M1 CPU | M1 CPU |

ê°„ë‹¨í•˜ê²Œ MNIST ë°ì´í„°ì…‹ì„ ë¶„ë¥˜í•˜ëŠ” torch ëª¨ë¸ì„ ì‘ì„±í–ˆë‹¤.

```python
import time
import os
import datetime
import math
import numpy as np
import torch
from torch.nn.functional import softmax
from torch import nn
from torch import optim
import torchvision
import torchvision.transforms as transforms

# device setting, nvidia='cuda:0' | m1='mps' | cpu='cpu'
DEVICE = 'cuda:0'    # "cuda:0" or "mps" or "cpu"

EPOCHS = 20
BATCH_SIZE = 16 
LEARNING_RATE = 0.0001

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()

        self.num_classes = 10
        self.ch = 1

        self.conv1 = nn.Conv2d(self.ch, 128, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(128)

        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(256)

        self.actv = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout2d(p=0.5, inplace=False)

        self.flatten = nn.Flatten()

        self.fn1 = nn.Linear(7 * 7 * 256, 256)
        self.fn2 = nn.Linear(256, 100)
        self.fn3 = nn.Linear(100, self.num_classes)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.actv(x)

        x = self.maxpool(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.actv(x)

        x = self.maxpool(x)
        x = self.dropout(x)

        x = self.flatten(x)
        x = self.fn1(x)
        x = self.fn2(x)
        x = self.fn3(x)

        return softmax(x)

train_set = torchvision.datasets.MNIST(
    root = './data/MNIST',
    train = True,
    download = True,
    transform = transforms.Compose([
        transforms.ToTensor() # ë°ì´í„°ë¥¼ 0ì—ì„œ 255ê¹Œì§€ ìˆëŠ” ê°’ì„ 0ì—ì„œ 1ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
    ])
)

test_set = torchvision.datasets.MNIST(
    root = './data/MNIST',
    train = False,
    download = True,
    transform = transforms.Compose([
        transforms.ToTensor() # ë°ì´í„°ë¥¼ 0ì—ì„œ 255ê¹Œì§€ ìˆëŠ” ê°’ì„ 0ì—ì„œ 1ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
    ])
)

model = Model()
n_params = sum(p.numel() for p in model.parameters())

print("\n===== Model Architecture =====")
print(model, "\n")

print("\n===== Model Parameters =====")
print(" - {}".format(n_params), "\n\n")

train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE)

total_train_iter = math.ceil(len(train_set) / BATCH_SIZE)
total_valid_iter = math.ceil(len(test_set) / BATCH_SIZE)

device = torch.device(DEVICE)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

model.to(device)

train_start = time.time()
epochs_times = []

print("\n\nTraining start time : {}\n\n".format(datetime.datetime.now()))

for epoch in range(EPOCHS):
    epoch_start = time.time()
    train_loss, train_acc = 0.0, 0.0

    for step, data in enumerate(train_loader):
        iter_start = time.time()
        model.train()
        image, target = data

        image  = image.to(device)
        target = target.to(device)

        out = model(image)

        acc = (torch.max(out, 1)[1].cpu().numpy() == target.cpu().numpy())
        acc = float(np.count_nonzero(acc) / BATCH_SIZE)

        loss = criterion(out, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        train_acc += acc

        if step % int(total_train_iter / 2) == 0:
            print("[train %5s/%5s] Epoch: %4s | Time: %6.2fs | loss: %10.4f | Acc: %10.4f" % (
                            step + 1, total_train_iter, epoch + 1, time.time() - iter_start, round(loss.item(), 4), float(acc)))

    train_loss = train_loss / total_train_iter
    train_acc = train_acc / total_train_iter
    epoch_runtime = time.time() - epoch_start
    print("[Epoch {} training Ended] > Time: {:.2}s/epoch | Loss: {:.4f} | Acc: {:g}\n".format(
        epoch + 1, epoch_runtime, np.mean(train_loss), train_acc))
    
    epochs_times.append(epoch_runtime)

program_runtime = time.time() - train_start

print("\n\nTraining running time : {:.2}\n\n".format(program_runtime))

epochs_times = list(map(str, epochs_times))
epochs_times = list(map(lambda x: str(x) + "\n", epochs_times))

filename = os.path.basename(__file__).split('.')[0] + ".txt"
with open(filename, 'w') as f:
    f.writelines(epochs_times)
print(f'save success epoch times! -> {filename}')
```

ì‹¤í—˜í•˜ë‹¤ë³´ë‹ˆ ì—„ì²­ë‚œ ë­”ê°€ê°€ ìˆì—ˆë‹¤â€¦

> ğŸ§ ì‹¤í—˜í•˜ë©´ì„œ í¬ìŠ¤íŒ… ì‘ì„± ì¤‘ì¸ë°, m1ì—ì„œ image size=224 ë¡œ í•˜ê³  batch_size=64ë¡œ ì„¤ì • í›„ì— í•™ìŠµì„ ì‹¤í–‰í•˜ë©´ ì“°ë¡œí‹€ë§ì´ ê±¸ë¦°ë‹¤!!! ì´ê²Œ ë§ë‚˜??  
> 
> ì•„ì§ ëŒ€ê·œëª¨ì˜ í•™ìŠµì€ ì œëŒ€ë¡œ ì˜ ì•ˆë˜ëŠ” ëŠë‚Œì´ë‹¤â€¦

* **1 Epoch ê²½ê³¼ ì‹œê°„ (M1 GPU vs Server GPU)**

```bash
(base) bolero is in now ~ $ python m1_gpu.py 

===== Model Architecture =====
Model(
  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (actv): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.5, inplace=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fn1): Linear(in_features=12544, out_features=256, bias=True)
  (fn2): Linear(in_features=256, out_features=100, bias=True)
  (fn3): Linear(in_features=100, out_features=10, bias=True)
) 

===== Model Parameters =====
 - 3535446 

Training start time : 2022-06-01 12:36:36.501451

/Users/bolero/m1_gpu.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return softmax(x)
[train     1/ 3750] Epoch:    1 | Time:   0.32s | loss:     2.3119 | Acc:     0.1250
[train  1876/ 3750] Epoch:    1 | Time:   0.03s | loss:     1.4933 | Acc:     0.9375
[Epoch 1 training Ended] > Time: 1.3e+02s/epoch | Loss: 1.5247 | Acc: 0.9397
```

***M1 Macbook Pro***

```bash
(neural) NvidiaServer@NvidiaServer:~$ python server_gpu.py 

===== Model Architecture =====
Model(
  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (actv): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.5, inplace=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fn1): Linear(in_features=12544, out_features=256, bias=True)
  (fn2): Linear(in_features=256, out_features=100, bias=True)
  (fn3): Linear(in_features=100, out_features=10, bias=True)
) 

===== Model Parameters =====
 - 3535446 

Training start time : 2022-06-01 03:36:49.959508

server_gpu.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return softmax(x)
[train     1/ 3750] Epoch:    1 | Time:   0.04s | loss:     2.3063 | Acc:     0.0000
[train  1876/ 3750] Epoch:    1 | Time:   0.01s | loss:     1.4613 | Acc:     1.0000
[Epoch 1 training Ended] > Time: 2.7e+01s/epoch | Loss: 1.5264 | Acc: 0.938167
```

***Nvidia RTX A6000***

> ğŸ§ 1 Epoch ê¹Œì§€ë§Œ ë´¤ì„ ë•Œ, Nvidia gpu ì„œë²„ìš©**(RTX A6000)** ì—ì„œëŠ” 27ì´ˆ~28ì´ˆ/epochê°€ ê±¸ë ¸ê³ , ***m1 ì—ì„œëŠ” 130ì´ˆ~140ì´ˆ/epochê°€ ê±¸ë ¸ë‹¤.*
ì•½ 5ë°°ì •ë„ ì°¨ì´ë‚˜ëŠ”ë°**, í˜„ì¬ ì„œë²„ìš© GPU ê°€ ë„ˆë¬´ ìš°ì›”í•œ ì ë„ ìˆë‹¤.

* **1 Epoch ê²½ê³¼ ì‹œê°„ (M1 GPU vs Server CPU)**

```bash
(base) bolero is in now ~ $ python m1_gpu.py 

===== Model Architecture =====
Model(
  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (actv): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.5, inplace=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fn1): Linear(in_features=12544, out_features=256, bias=True)
  (fn2): Linear(in_features=256, out_features=100, bias=True)
  (fn3): Linear(in_features=100, out_features=10, bias=True)
) 

===== Model Parameters =====
 - 3535446 

Training start time : 2022-06-01 12:36:36.501451

/Users/bolero/m1_gpu.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return softmax(x)
[train     1/ 3750] Epoch:    1 | Time:   0.32s | loss:     2.3119 | Acc:     0.1250
[train  1876/ 3750] Epoch:    1 | Time:   0.03s | loss:     1.4933 | Acc:     0.9375
[Epoch 1 training Ended] > Time: 1.3e+02s/epoch | Loss: 1.5247 | Acc: 0.9397
```

***M1 Macbook Pro***

```bash
(neural) NvidiaServer@NvidiaServer:~$ python server_cpu.py 

===== Model Architecture =====
Model(
  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (actv): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.5, inplace=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fn1): Linear(in_features=12544, out_features=256, bias=True)
  (fn2): Linear(in_features=256, out_features=100, bias=True)
  (fn3): Linear(in_features=100, out_features=10, bias=True)
) 

===== Model Parameters =====
 - 3535446 

Training start time : 2022-06-01 03:37:22.395249

server_cpu.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return softmax(x)
[train     1/ 3750] Epoch:    1 | Time:   1.38s | loss:     2.3021 | Acc:     0.0625
[train  1876/ 3750] Epoch:    1 | Time:   0.04s | loss:     1.4617 | Acc:     1.0000
[Epoch 1 training Ended] > Time: 1.8e+02s/epoch | Loss: 1.5256 | Acc: 0.939617
```

***Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz * 40 Core***

> ğŸ§ ë‹¤í–‰ìŠ¤ëŸ½ê²Œë„(?) **M1 GPUê°€ Serverì˜ 40 Core CPUë³´ë‹¤ëŠ” ë¹¨ëë‹¤.**
ìˆ˜ì¹˜ë¡œ ë³´ìë©´ M1 GPUê°€ 130ì´ˆ/epoch ì •ë„ ë‚˜ì˜¤ê³ , 40 Core CPUê°€ 180ì´ˆ/epoch ì •ë„ ë‚˜ì˜¨ë‹¤. (**ì•½ 1.4ë°°ì •ë„ M1 GPUê°€ ë” ë¹ ë¦„**)

1. **1 Epoch ê²½ê³¼ ì‹œê°„ (M1 CPU vs M1 GPU)**

```bash
(base) bolero is in now ~ $ python m1_cpu.py 

===== Model Architecture =====
Model(
  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (actv): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.5, inplace=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fn1): Linear(in_features=12544, out_features=256, bias=True)
  (fn2): Linear(in_features=256, out_features=100, bias=True)
  (fn3): Linear(in_features=100, out_features=10, bias=True)
) 

===== Model Parameters =====
 - 3535446 

Training start time : 2022-06-01 12:36:37.141268

/Users/bolero/m1_cpu.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return softmax(x)
[train     1/ 3750] Epoch:    1 | Time:   0.09s | loss:     2.3023 | Acc:     0.1250
[train  1876/ 3750] Epoch:    1 | Time:   0.05s | loss:     1.4612 | Acc:     1.0000
[Epoch 1 training Ended] > Time: 1.7e+02s/epoch | Loss: 1.5262 | Acc: 0.938867
```

***M1 Macbook Pro - CPU***

```bash
(base) bolero is in now ~ $ python m1_gpu.py 

===== Model Architecture =====
Model(
  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (actv): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.5, inplace=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (fn1): Linear(in_features=12544, out_features=256, bias=True)
  (fn2): Linear(in_features=256, out_features=100, bias=True)
  (fn3): Linear(in_features=100, out_features=10, bias=True)
) 

===== Model Parameters =====
 - 3535446 

Training start time : 2022-06-01 12:36:36.501451

/Users/bolero/m1_gpu.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return softmax(x)
[train     1/ 3750] Epoch:    1 | Time:   0.32s | loss:     2.3119 | Acc:     0.1250
[train  1876/ 3750] Epoch:    1 | Time:   0.03s | loss:     1.4933 | Acc:     0.9375
[Epoch 1 training Ended] > Time: 1.3e+02s/epoch | Loss: 1.5247 | Acc: 0.9397
```

***M1 Macbook Pro - GPU***

> ğŸ§ ì†”ì§íˆ M1-CPU ë‚˜ M1-GPU ë‚˜ ë­”ê°€ ì½”ë”© ì˜ëª»í–ˆì„ ì¤„ ì•Œê³  ê¸°ëŒ€ ì•ˆí–ˆëŠ”ë°, ì˜ì™¸ë¡œ ì°¨ì´ê°€ ë°œìƒí–ˆë‹¤.
**M1-CPUê°€ 170ì´ˆ~180ì´ˆ/epoch ì •ë„ ë‚˜ì˜¤ê³ , M1-GPUëŠ” 130ì´ˆ/epoch ì •ë„ ë‚˜ì˜¤ëŠ”ë°, M1-CPU ì˜ ìˆ˜ì¤€ì´ Intel Xeon 4210R 2.40GHz 40 Core ì •ë„ì˜ ìˆ˜ì¤€ì„ ë³´ì˜€ë‹¤.**


ê²°ë¡ ì€â€¦ M1-GPU ëŠ” ì—„ì²­ ì“¸ ì •ë„ëŠ” ì•„ë‹ˆë‹¤ ì•„ì§! ê·¸ë˜ë„ CPUë³´ë‹¤ëŠ” ë‚˜ìœ¼ë‹ˆ ê°„ë‹¨í•œ í…ŒìŠ¤íŒ… ì •ë„ëŠ” ë¬´ë¦¬ì—†ì´ ëŒë¦´ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë¨.

**(ë‹¨ì ì€, ì•„ê¹Œ ì‹¤í—˜í•´ë´¤ëŠ”ë° `image_size=[224, 224]` ë¡œ í•˜ê³  `batch_size=64` ë¡œ í•˜ë‹ˆê¹Œ macbookì— 5~7ì´ˆë§ˆë‹¤ ì“°ë¡œí‹€ë§ì´ ë°œìƒí–ˆë‹¤â€¦ğŸ¥²)**

í•™ìŠµ ì¢…ë£Œ ë•Œ ì‹¤í—˜ ë°©ì‹ ë³„ ë§¤ Epochì˜ ì†Œìš”ì‹œê°„ì„ ì €ì¥í•˜ëŠ”ë°, ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ëŠ” ê±¸ë¡œ í¬ìŠ¤íŒ…ì„ ë§ˆë¬´ë¦¬ í•˜ê² ë‹¤.
(server CPUëŠ” ì„œë²„ ë¨¸ì‹ ì˜ ë‹¤ë¥¸ ì‘ì—…ë•Œë¬¸ì— 20 Epochsê¹Œì§€ ì¸¡ì •í•˜ì§€ ëª»í•¨.)

**ì†ŒìŠ¤ì½”ë“œ**

```python
from matplotlib import pyplot as plt

server_gpu = [0 for x in range(0, 20)]
m1_gpu = [0 for x in range(0, 20)]
m1_cpu = [0 for x in range(0, 20)]

epochs = [x for x in range(1, 21)]

with open("server_gpu.txt", 'r') as f:
    server_gpu = f.readlines()
    server_gpu = list(map(float, server_gpu))

with open("m1_gpu.txt", 'r') as f:
    m1_gpu = f.readlines()
    m1_gpu = list(map(float, m1_gpu))

with open("m1_cpu.txt", 'r') as f:
    m1_cpu = f.readlines()
    m1_cpu = list(map(float, m1_cpu))

plt.figure()
plt.title('Training time per devices')

plt.plot(epochs, server_gpu, marker='o', color='green')
plt.plot(epochs, m1_gpu, marker='o', color='red')
plt.plot(epochs, m1_cpu, marker='o', color='black')

plt.legend(['Server GPU', 'Server CPU', 'M1 GPU', 'M1 CPU'])
plt.xlabel("Epochs")
plt.ylabel("Running time(seconds)")
# plt.show()
plt.savefig("runtime.png")
```

**ê²°ê³¼ ì´ë¯¸ì§€**  
![](https://velog.velcdn.com/images/bolero2/post/b04b61e8-e95a-46be-b214-049e490f2955/image.png)


