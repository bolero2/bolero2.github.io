<!DOCTYPE html>




<html
 dir="ltr"
 lang="en"
 >
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content=#ffffff>
    <link rel="stylesheet" href="/assets/css/app.css">
    <link rel="shortcut icon" type="image/png"
           href="/favicon.png" 
    />
    <script defer src="https://unpkg.com/alpinejs@3.9.0/dist/cdn.min.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-social@1/bin/bulma-social.min.css">
    
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>[Paper] DETR - End-to-End Object Detection with Transformer | bolero2.log</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="[Paper] DETR - End-to-End Object Detection with Transformer" />
<meta name="author" content="bolero2" />
<meta property="og:locale" content="en" />
<meta name="description" content="Facebook AI 팀에서 발표한, Transformer를 Object Detection task에 사용한 논문입니다." />
<meta property="og:description" content="Facebook AI 팀에서 발표한, Transformer를 Object Detection task에 사용한 논문입니다." />
<link rel="canonical" href="http://localhost:4000/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/" />
<meta property="og:url" content="http://localhost:4000/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/" />
<meta property="og:site_name" content="bolero2.log" />
<meta property="og:image" content="http://localhost:4000/img/detr.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-12T00:00:00+09:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/img/detr.png" />
<meta property="twitter:title" content="[Paper] DETR - End-to-End Object Detection with Transformer" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"bolero2"},"dateModified":"2022-01-12T00:00:00+09:00","datePublished":"2022-01-12T00:00:00+09:00","description":"Facebook AI 팀에서 발표한, Transformer를 Object Detection task에 사용한 논문입니다.","headline":"[Paper] DETR - End-to-End Object Detection with Transformer","image":"http://localhost:4000/img/detr.png","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/"},"url":"http://localhost:4000/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/"}</script>
<!-- End Jekyll SEO tag -->

    <!-- head scripts --></head>

  <body>
    <nav class="navbar is-primary " x-data="{ openNav: false }">
    <div class="container">
        <div class="navbar-brand">
            <a href="/" class="navbar-item">
                bolero2.log
            </a>
            <a role="button" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navMenu" :class="{ 'is-active': openNav }" x-on:click="openNav = !openNav">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu" id="navMenu" :class="{ 'is-active': openNav }">
            <div class="navbar-start">
                <a href="/" class="navbar-item ">Home</a>
                
                
                    
                    <a href="/docs/" class="navbar-item ">Docs</a>
                    
                
                    
                    <div class="navbar-item has-dropdown is-hoverable ">
                        <a href="/" class="navbar-link ">Example Pages</a>
                        <div class="navbar-dropdown">
                            
                            <a href="/landing/" class="navbar-item ">Landing Page With Callouts</a>
                            
                            <a href="/sponsors/" class="navbar-item ">Sponsors Page</a>
                            
                            <a href="/gallery/" class="navbar-item ">Image Gallery</a>
                            
                            <a href="/products/" class="navbar-item ">Products</a>
                            
                            <a href="/example-recipe/" class="navbar-item ">Recipe Page</a>
                            
                            <a href="/showcase/" class="navbar-item ">Showcase</a>
                            
                        </div>
                    </div>
                    
                
                    
                    <a href="/blog/" class="navbar-item ">Blog</a>
                    
                
                
            </div>

            <div class="navbar-end">
                
                <a class="navbar-item" href="https://github.com/sponsors/chrisrhymes">
                    <span class="icon gh-sponsor"><i class="fas fa-heart"></i></span>
                    <span>Sponsor</span>
                </a>
                
            </div>

        </div>
    </div>
</nav>

    
        <section class="hero  is-medium  is-bold is-primary" >
    <div class="hero-body ">
        <div class="container">
            <h1 class="title is-2">[Paper] DETR - End-to-End Object Detection with Transformer</h1>
            <p class="subtitle is-3"></p>
            
        </div>
    </div>
</section>
    
    


    <section class="section">
        <div class="container">
            <div class="columns is-multiline">
                
                <div class="column is-8">
                    
                    
                    
                    
                    <div class="content">

    <p>Published: Jan 12, 2022 by bolero2</p>

    

    <h1 id="detr---end-to-end-object-detection-with-transformer">DETR - End-to-End Object Detection with Transformer</h1>

<p>이번에 소개할 논문은 Facebook AI 팀에서 공개한<br />
Transformer 방식을 Computer Vision의 Object Detection 분야에 적용시킨 <strong>DETR</strong>입니다.</p>

<p>DETR은 <strong>DE</strong>tection + <strong>TR</strong>ansformer 의 줄임말로, 이름에서부터 Transformer가 Detection 방식에 사용됨을 유추할 수 있습니다.</p>

<p>논문 제목에서 <strong>End-to-End</strong> 라는 말의 의미는,<br />
(뒤에 등장하지만)기존 Detection Network가 가지고 있는 초매개변수(<strong>Hyper-Parameter</strong>, ex. NMS, threshold, anchor-box etc.)를<br />
Transformer의 End-to-End 방식의 학습을 통해 없앴다고 볼 수 있습니다.</p>

<hr />

<h2 id="1-abstract">1. Abstract</h2>

<p>논문에서 크게 주장하는 핵심은 다음과 같습니다:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. 사용자가 설정해야 하는 것(Hand-designed Components) 을 제거  
2. Simple한 Network 구성  
3. 이분법적 매칭(Bipartite Matching)과 Transformer의 Encoder-Decoder 구조 사용  
</code></pre></div></div>
<p>추가적으로, Object Detection 분야 뿐 만 아니라<br />
<strong>Panoptic Segmentation(a.k.a Instance Segmentation)</strong> 분야에서도 좋은 성능을 보여준다고 합니다.</p>

<hr />

<h2 id="2-model-architecture">2. Model Architecture</h2>

<p>네트워크의 전체적인 구성은 다음과 같습니다:</p>

<p><img src="https://user-images.githubusercontent.com/41134624/105303525-e6fb9400-5bfe-11eb-947c-ef4939938df6.jpg" alt="model1" /></p>

<p>해당 네트워크는 크게 본다면</p>
<blockquote>
  <p>1) <strong>C</strong>onvolution <strong>N</strong>eural <strong>N</strong>etwork(ResNet)
2) <strong>Transformer</strong> Encoder
3) <strong>Transformer</strong> Decoder
4) <strong>F</strong>eed-<strong>F</strong>oward <strong>N</strong>etwork(FFN)</p>
</blockquote>

<p>이렇게 4단계로 구분할 수 있습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DETR은 이 4단계를 통해 입력 데이터를 곧바로 분류 정보 및 bbox 정보를 추론 하기 때문에,  
NMS와 같은 사용자의 입력 값을 요구하는 알고리즘이 필요하지 않습니다.
</code></pre></div></div>

<h3 id="1-convolution-neural-network">1) Convolution Neural Network</h3>

<p>CNN의 주 목적은 입력 영상 데이터의 <em><strong>특징 추출</strong></em> 입니다.<br />
논문에서 사용한 CNN(=Backbone)은 ResNet으로,<br />
<strong>3ch * W * H</strong> 영상 데이터가 입력으로 들어온 후 &gt; 최종 <strong>2048ch * W/32 * H/32</strong> 크기의 Feature Map을 생성합니다.</p>

<p><img src="https://user-images.githubusercontent.com/41134624/105318677-262de300-5c07-11eb-983c-c26c68abe782.jpg" alt="resnet" /></p>

<p>저자는 Backbone CNN으로 ResNet50 모델을 사용하였는데,<br />
해당 모델의 맨 마지막 channel 깊이는 <strong>2048</strong>임을 알 수 있습니다.</p>

<h3 id="2-transformer-encoder">2) Transformer Encoder</h3>

<p>CNN을 거쳐 생성된 Feature Map은 1x1 convolution을 통해 <strong>d 차원</strong>(=d 채널)으로 축소됩니다.</p>
<blockquote>
  <ul>
    <li><em><strong>Encoder는 Sequence Data를 입력으로 받기 때문에, Vectorizing함을 알 수 있습니다.</strong></em></li>
    <li><em><strong>또한, 축소 된 d 채널은 Spatial하게 분리하여 H*W 크기로 구성된 d 개의 조각으로 분리할 수 있습니다.</strong></em></li>
  </ul>
</blockquote>

<p>2) Transformer Encoder
CNN을 거쳐 생성된 Feature Map은 1x1 convolution을 통해 d 차원(=d 채널)으로 축소됩니다.</p>

<p>Encoder는 Sequence Data를 입력으로 받기 때문에, Vectorizing함을 알 수 있습니다.
또한, 축소 된 d 채널은 Spatial하게 분리하여 H*W 크기로 구성된 d 개의 조각으로 분리할 수 있습니다.</p>

<p>각각의 d개 조각은 Encoder Layer의 입력으로 Sequencial하게 들어가며, Encoder Layer는 기본적인 구조로 구성되어 있습니다.</p>
<blockquote>
  <ul>
    <li><em><strong>Encoder Layer는 Multi-head Self-attention module로 구성되어 있습니다.</strong></em></li>
  </ul>
</blockquote>

<p>Encoder에서 살펴 볼 것은 다음과 같습니다:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* 원래 Transformer는 입력 데이터의 순서가 출력 데이터에 영향을 주지 않습니다.
* 하지만 Vision 문제에서는 분리 된 d개의 조각에 대한 순서다가 중요하기 때문에 각각의 Attention Layer마다 Position Embedding을 실시합니다.
</code></pre></div></div>

<h3 id="3-transformer-decoder">3) Transformer Decoder</h3>

<p>Decoder 역시 Encoder와 동일하게 Standard한 구조를 따릅니다.<br />
Encoder의 출력으로 d size의 N Embedding이 나오고, 이는 그대로 Decoder의 입력으로 들어갑니다.</p>

<p>Decoder에서 살펴 볼 것은 다음과 같습니다:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>* 원래의 Decoder는 분리 된 d 개의 조각을 하나의 Sequence로 보고, 통째로 입력 데이터로 들어갑니다.
* 하지만 DETR에서는 각각의 Decoder Layer마다 N 개의 Embedding 객체를 Parallel하게 Decoding합니다.
* 또한, Encoder처럼 각각의 Attention Layer에 Object Query를 추가하여 Position Embedding과 유사한 작업을 합니다.
</code></pre></div></div>

<h3 id="4-feed-foward-networkffn">4) Feed-Foward Network(FFN)</h3>

<p>FFN 같은 경우는 단순한 구조로 되어 있습니다:</p>
<blockquote>
  <ul>
    <li>3 Layer의 Perceptron으로 구성되어 있습니다.</li>
    <li>각각의 Perceptron은 ReLU 활성화 함수와 d 차원의 은닉층, 1개의 Linear Projection으로 되어 있습니다.</li>
  </ul>
</blockquote>

<p>또한, FFN을 거치게 되면 Predict한 값이 나오게 되는데, 이 값은 다음과 같습니다:</p>
<blockquote>
  <p>1) Center X (relative)
2) Center Y (relative)
3) Height (relative)
4) Width (relative)</p>
</blockquote>

<p>(Relative한 좌표는 픽셀의 개수를 count하는 절대 좌표가 아닌, 이미지 전체의 H/W에 비례하는 0과 1사이의 좌표 값입니다.)</p>

<p>FFN은 Softmax 함수를 통해 분류 라벨 또한 Predict 합니다.<br />
Predict 할 때, Ground-Truth 개수가 5개이고, Detection 객체 개수가 7개라면<br />
Ground-Truth 쪽에 2개의 (no object)를 만들어줍니다.</p>

<p><img src="https://user-images.githubusercontent.com/41134624/105328912-e5889680-5c13-11eb-8a99-9e9e822a3da2.jpg" alt="label_predict" /></p>

<p>그림과 같이, 4개의 객체를 검출했다면, 2개는 (no object) 항목으로 할당하고 2개는 정답으로 처리하여 <strong>이분법(bipartite)적으로 처리하게 됩니다.</strong></p>

<hr />

<h2 id="3-experiments">3. Experiments</h2>

<p>실험 조건은 다음과 같습니다:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Item</th>
      <th style="text-align: center">Content</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Comparison Network</td>
      <td style="text-align: center">Faster-RCNN</td>
    </tr>
    <tr>
      <td style="text-align: center">Optimizer</td>
      <td style="text-align: center">AdamW</td>
    </tr>
    <tr>
      <td style="text-align: center">Backbone</td>
      <td style="text-align: center">ResNet-50, ResNet-101</td>
    </tr>
    <tr>
      <td style="text-align: center">Epoch</td>
      <td style="text-align: center">300</td>
    </tr>
    <tr>
      <td style="text-align: center">Dataset</td>
      <td style="text-align: center">COCO 2017</td>
    </tr>
  </tbody>
</table>

<p>실험에 사용된 Dataset은 <a href="https://cocodataset.org/#home">COCO 2017</a>의 detection + segmentation 데이터 세트 입니다.<br />
Segmentation은 Panoptic Segmentation의 성능 측정을 위해 사용하였습니다.</p>

<p><img src="https://user-images.githubusercontent.com/41134624/105329673-bf172b00-5c14-11eb-8ca7-468b4020761e.jpg" alt="exp1" /></p>

<p>그림에서 보는 것과 같이,</p>
<ol>
  <li>대부분의 상황에서 DETR의 parameter 개수가 현저히 낮음을 알 수 있으며,</li>
  <li>Average Precision은 6 case 중 4 case에서 Faster-RCNN보다 높음을 알 수 있습니다.</li>
</ol>

<p>여기서 Faster-RCNN이 높은 케이스 중, AP-Small size는 Faster-RCNN이 27.2로 23.7의 DETR보다 우월하게 높습니다.</p>

<p><strong>즉, DETR은 작은 Object에 대해서 상대적으로 약함을 보입니다.</strong></p>

<hr />

<h2 id="4-source-code">4. Source Code</h2>

<p>논문의 저자는 Paper 맨 뒤에 간단한 구현 코드를 공개했습니다.<br />
Abstract에서 말한 것처럼, 코드는 매우 간단한 구조로 되어 있습니다:</p>

<p><img src="https://user-images.githubusercontent.com/41134624/105314318-75711500-5c01-11eb-98b2-90eae749d3d0.jpg" alt="code" /></p>

<p>이 코드에서 우리는 ResNet-50 모델을 사용한 것과, 내부 프레임워크에서 제공하는 수준의 Transformer 함수를 그대로 사용한 것을 알 수 있습니다.</p>

<hr />

<p><strong>(Paper Review는 제가 스스로 읽고 작성한 글이므로, 주관적인 내용임을 밝힙니다.)</strong></p>

</div>

<div class="tags">
    
</div>


<p><strong>Share</strong></p>
<div class="buttons ">
    <a class="button is-medium is-facebook"
       onclick="window.open('https://www.facebook.com/share.php?u=http://localhost:4000/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/');">
        <span class="icon"><i class="fab fa-facebook fa-lg"></i></span>
    </a>
    <a class="button is-medium is-twitter"
       onclick="window.open('https://twitter.com/intent/tweet?text=http://localhost:4000/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/');">
        <span class="icon"><i class="fab fa-twitter fa-lg"></i></span>
    </a>
    <a class="button is-medium is-linkedin"
       onclick="window.open('https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/&title=%5BPaper%5D+DETR+-+End-to-End+Object+Detection+with+Transformer&summary=&source=');">
        <span class="icon"><i class="fab fa-linkedin fa-lg"></i></span>
    </a>
    <a class="button is-medium is-reddit"
       onclick="window.open('https://reddit.com/submit?url=http://localhost:4000/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/');">
        <span class="icon"><i class="fab fa-reddit fa-lg"></i></span>
    </a>
</div>



  


                </div>
                
                <div class="column is-4-desktop is-4-tablet">
                    <p class="title is-4">Latest Posts</p>

<div class="columns is-multiline">
    
    <div class="column is-12">
        <div class="card">
    
    <div class="card-image">
        <img src="../img/Better_plain_ViT_baselines_for_ImageNet-1k.png" alt="[Papers] Better  plain ViT baselines for ImageNet-1k">
    </div>
    
    <div class="card-content">
        <div class="content">
            
            <a class="title is-4" href="/paper/2022/05/03/Papers-Better-plain-ViT-baselines-for-ImageNet-1k/">[Papers] Better  plain ViT baselines for ImageNet-1k</a>
            
            
                <p><h1 id="better--plain-vit-baselines-for-imagenet-1k">Better  plain ViT baselines for ImageNet-1k</h1>
</p>
            
        </div>
        <div class="has-text-centered">
            <a href="/paper/2022/05/03/Papers-Better-plain-ViT-baselines-for-ImageNet-1k/" class="button is-primary">Read more</a>
        </div>
    </div>
    <footer class="card-footer">
        <p class="card-footer-item">Published: May 3, 2022</p>
    </footer>
</div>
    </div>
    
    <div class="column is-12">
        <div class="card">
    
    <div class="card-image">
        <img src="../img/segmentation_label.png" alt="[DL] Semantic Segmentation에서 Label Image 생성하기">
    </div>
    
    <div class="card-content">
        <div class="content">
            
            <a class="title is-4" href="/deeplearning/2022/01/15/DL-Semantic-Segmentation%EC%97%90%EC%84%9C-Label-Image-%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0/">[DL] Semantic Segmentation에서 Label Image 생성하기</a>
            
            
                <p><h2 id="-0intro-">** 0.Intro **</h2>
<p>Semantic Segmentation Task를 학습하는데는 2가지 데이터가 필요합니다:
<strong><em>(Input) X Data : 학습할 Image dataset
(Input) Y Data : 학습할 Image에 대응되는 Color Map Image file</em></strong></p>
</p>
            
        </div>
        <div class="has-text-centered">
            <a href="/deeplearning/2022/01/15/DL-Semantic-Segmentation%EC%97%90%EC%84%9C-Label-Image-%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0/" class="button is-primary">Read more</a>
        </div>
    </div>
    <footer class="card-footer">
        <p class="card-footer-item">Published: Jan 15, 2022</p>
    </footer>
</div>
    </div>
    
    <div class="column is-12">
        <div class="card">
    
    <div class="card-image">
        <img src="../img/detr.png" alt="[Paper] DETR - End-to-End Object Detection with Transformer">
    </div>
    
    <div class="card-content">
        <div class="content">
            
            <a class="title is-4" href="/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/">[Paper] DETR - End-to-End Object Detection with Transformer</a>
            
            
                <p><h1 id="detr---end-to-end-object-detection-with-transformer">DETR - End-to-End Object Detection with Transformer</h1>
</p>
            
        </div>
        <div class="has-text-centered">
            <a href="/paper/2022/01/12/Paper-DETR-End-to-End-Object-Detection-with-Transformer/" class="button is-primary">Read more</a>
        </div>
    </div>
    <footer class="card-footer">
        <p class="card-footer-item">Published: Jan 12, 2022</p>
    </footer>
</div>
    </div>
    
</div>




                </div>
                
            </div>
        </div>
    </section>
    
        <footer class="footer">
    <div class="container">
        
        
        <div class="columns is-multiline">
            
            <div class="column has-text-centered">
                <div>
                    <a href="/" class="link">Home</a>
                </div>
            </div>
            
            <div class="column has-text-centered">
                <div>
                    <a href="/blog/" class="link">Blog</a>
                </div>
            </div>
            
            <div class="column has-text-centered">
                <div>
                    <a href="/products/" class="link">Products</a>
                </div>
            </div>
            
            <div class="column has-text-centered">
                <div>
                    <a href="/privacy-policy/" class="link">Privacy Policy</a>
                </div>
            </div>
            
        </div>
        

        <div class="content is-small has-text-centered">
            <p class="">Theme built by <a href="https://www.csrhymes.com">C.S. Rhymes</a></p>
        </div>
    </div>
</footer>

    
    <script src="/assets/js/app.js" type="text/javascript"></script><!-- footer scripts --></body>
</html>
