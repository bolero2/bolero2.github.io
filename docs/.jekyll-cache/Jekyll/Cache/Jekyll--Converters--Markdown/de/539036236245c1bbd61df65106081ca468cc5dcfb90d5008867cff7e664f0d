I"5<p><span class="tag  is-primary ">
    DeepLearning
</span></p>

<h1 id="apple-siliconì—-pytorch-ì„¤ì¹˜í•˜ê¸°">Apple Siliconì— PyTorch ì„¤ì¹˜í•˜ê¸°</h1>

<p>ì•„ë§ˆ 2020ë…„ì´ì—ˆë‚˜â€¦ ì• í”Œ ì‹¤ë¦¬ì½˜ ì•„í‚¤í…ì³ì˜ Macbookì´ ì¶œì‹œë˜ì—ˆë‹¤.</p>

<p>ë‚˜ëŠ” í˜„ì¬ M1 macbook proë¥¼ ì“°ê³ ìˆëŠ”ë°, ì´ì „ì—ëŠ” anacondaë„ ì•ˆë˜ì„œ mini-forgeë¼ëŠ” ê±¸ ì‚¬ìš©í–ˆì—ˆë‹¤.</p>

<p>ê·¸ë¦¬ê³  ê·¸ mini-forgeì—ëŠ” ë§ë„ ì•ˆë˜ëŠ” ë°˜ìª½ì§œë¦¬ pytorchë¥¼ ì„¤ì¹˜í•´ì„œ ì‚¬ìš©í–ˆì—ˆë‹¤.</p>

<p>ì´ pytorchê°€ êµ¬ë™ì´ ì•ˆë˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ì–´ì°Œì–´ì°Œí•´ì„œ ì„¤ì¹˜í•´ì„œ í•˜ë©´ ë­”ê°€ ë˜ê¸´í–ˆì—ˆëŠ”ë° GPU ê°€ì†ë„ ì•ˆë˜ê³  ì—¬ëŸ¬ ì œì•½ì‚¬í•­ì´ ìˆì—ˆë‹¤.
(ë¬¼ë¡  ê°œë°œì€ ì„œë²„ê°€ ìˆê¸° ë•Œë¬¸ì— ë”±íˆ ìƒê´€ì€ ì—†ì—ˆëŠ”ë°â€¦ ê°€ë²¼ìš´ ê²ƒì´ë¼ë„ ì˜ ëŒë¦¬ê³  ì‹¶ê¸´ í•˜ë‹ˆê¹Œ?!)</p>

<p>í•˜ì§€ë§Œ! ë“œë””ì–´</p>

<blockquote>
  <p>ğŸ’¡ <strong>Apple Silicon(M1) ì„ ìœ„í•œ Anaconda ì™€ PyTorchê°€ ì •ì‹ ì¶œì‹œë˜ì—ˆë‹¤!</strong></p>
</blockquote>

<ul>
  <li>Pytorch ê´€ë ¨ ê¸°ì‚¬ : <a href="https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/?fbclid=IwAR25rWBO7pCnLzuOLNb2rRjQLP_oOgLZmkJUg2wvBdYqzL72S5nppjg9Rvc">Introducing Accelerated PyTorch Training on Mac</a></li>
</ul>

<p>ì•„ì§ Preview ë²„ì „ì´ë¼ëŠ”ê±°ë‹ˆê¹Œ ì •ì‹(?)ì€ ì•„ë‹Œê±¸ë¡œâ€¦</p>

<h2 id="1-anaconda-ì„¤ì¹˜">1. Anaconda ì„¤ì¹˜</h2>

<p>Anaconda ì„¤ì¹˜ëŠ” ë§¤ìš° ì‰½ë‹¤. Anaconda ê³µì‹ í™ˆí˜ì´ì§€ì—ì„œ pkg íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•˜ì—¬ ê·¸ëƒ¥ ë”ë¸”í´ë¦­í•˜ê³  ì„¤ì¹˜í•˜ë©´ ëœë‹¤.</p>

<ol>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Anaconda download page ë¥¼ ë“¤ì–´ê°„ë‹¤.</strong> : [Anaconda</td>
          <td>Anaconda Distribution](https://www.anaconda.com/products/distribution)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p><strong>ì‚¬ê³¼ ì•„ì´ì½˜ì„ í´ë¦­í•œë‹¤. (ë§¨ ì•„ë˜ë¡œ ì´ë™ë¨.)</strong>
<img src="https://velog.velcdn.com/images/bolero2/post/ee7a8a87-acfe-4614-8d50-5f070c35627f/image.png" alt="" /></p>
  </li>
  <li>
    <p><strong>ë§¨ ì•„ë˜ì—ì„œ 64-Bit(M1) Graphical Installer (428MB) ë¥¼ í´ë¦­í•œë‹¤.</strong>
<img src="https://velog.velcdn.com/images/bolero2/post/0ef75259-038b-4a76-ae3e-51b78c344a8f/image.png" alt="" /></p>
  </li>
  <li><strong>ê·¸ëŸ¬ë©´ ì•„ë˜ì²˜ëŸ¼ pkg íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ëœë‹¤. (arm64=apple silicon ì•„í‚¤í…ì³ì„.)</strong>
<img src="https://velog.velcdn.com/images/bolero2/post/4dcbb047-29ca-4bc6-b7df-39c90a463d99/image.png" alt="" /></li>
</ol>

<p>pkg íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œê°€ ì™„ë£Œë˜ë©´, ì•„ë˜ì²˜ëŸ¼ ë­”ê°€ ê²½ê³ ë„ ëœ¨ê³  ê³„ì•½ì„œ ë™ì˜ë„ í•˜ê³ , ë””ìŠ¤í¬ ì§€ì •ë„ í•´ì¤˜ì•¼ í•œë‹¤.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">install-1</th>
      <th style="text-align: center">install-2</th>
      <th style="text-align: center">install-3</th>
      <th style="text-align: center">install-4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://velog.velcdn.com/images/bolero2/post/c5a7da7a-26f5-428e-b020-42fdcf8481a3/image.png" alt="" /></td>
      <td style="text-align: center"><img src="https://velog.velcdn.com/images/bolero2/post/6530967f-749b-4628-93a7-964aae4eed79/image.png" alt="" /></td>
      <td style="text-align: center"><img src="https://velog.velcdn.com/images/bolero2/post/5b9e3851-29f4-45b7-8b9a-5ea2d2c76109/image.png" alt="" /></td>
      <td style="text-align: center"><img src="https://velog.velcdn.com/images/bolero2/post/3a568b29-7b45-4645-9e60-f37c5ba19c93/image.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<p>ê·¸ë¦¬ê³ ëŠ” ì„¤ì¹˜í•˜ë©´ ëœë‹¤â€¦</p>

<ul>
  <li>ì„¸íŒ…ë„ ìë™ìœ¼ë¡œ í•´ì¤˜ì„œ ë§¤~ìš° í¸í•˜ë‹¤.
<img src="https://velog.velcdn.com/images/bolero2/post/f6fd127e-f3a8-48b4-9ce3-400b74ad9b25/image.png" alt="" /></li>
</ul>

<blockquote>
  <p>Anaconda ê°€ ì˜ ëœ¨ëŠ”ì§€ í™•ì¸í•˜ì.
matplotlib íŒ¨í‚¤ì§€ê°€ ì˜ import ë˜ëŠ” ê±¸ ë³´ë‹ˆ Anacondaê°€ í‹€ë¦¼ì—†ë‹¤â€¦</p>
</blockquote>

<ul>
  <li>conda init ì„¸íŒ…ì€ <code class="language-plaintext highlighter-rouge">vi ~/.zshrc</code> ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.
<img src="https://velog.velcdn.com/images/bolero2/post/fcd3c298-dfe6-48b6-b3e4-9c3a1c5624e2/image.png" alt="" /></li>
</ul>

<blockquote>
  <p>/Users/{username}/opt/anaconda3 ì— ì‹¤ì œ íŒŒì¼ë“¤ì´ ë“¤ì–´ìˆë‹¤.
ì‚­ì œí•˜ê³  ì‹¶ìœ¼ë©´ í•´ë‹¹ í´ë”ë¥¼ ì‚­ì œí•´ì£¼ì.</p>
</blockquote>

<h1 id="2-pytorch-ì„¤ì¹˜">2. PyTorch ì„¤ì¹˜</h1>

<p>Anaconda m1 ë²„ì „ì„ ì˜ ì„¤ì¹˜í–ˆìœ¼ë‹ˆ, PyTorchë¥¼ ì„¤ì¹˜í•´ì£¼ì.</p>

<p>PyTorch ì„¤ì¹˜ëŠ” ì •ë§ ë§¤ìš° ì‰½ë‹¤.</p>

<p>ìš°ì„ , <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a> ë¡œ ê°€ì„œ ì„¤ì¹˜ ì»¤ë§¨ë“œë¥¼ ì•Œì•„ì•¼ í•œë‹¤.
<img src="https://velog.velcdn.com/images/bolero2/post/6003e2bf-22f6-4125-9494-41021b0e5bf7/image.png" alt="" /></p>

<p>(ì•„ê¹Œ ê³µì‹ ì¹¼ëŸ¼ì—ì„œ ë³´ì•˜ë“¯ì´, Nightly ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜í•´ì•¼ í•œë‹¤.)</p>

<p><code class="language-plaintext highlighter-rouge">pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu </code></p>

<p>ë¡œ ì„¤ì¹˜í•˜ë©´ ëœë‹¤. ë°”ë¡œ ì„¤ì¹˜í•´ë³´ì.
<img src="https://velog.velcdn.com/images/bolero2/post/b13e16ec-d7de-4ad3-9ed4-733aac871746/image.png" alt="" /></p>

<p>PyTorch ì„¤ì¹˜ë„ ëë‚¬ë‹¤.</p>

<h1 id="3-pytorch-in-m1---gpu-acceleration">3. PyTorch in M1 - GPU Acceleration</h1>

<p>ì¼ë°˜ì ì¸ Nvidia gpuë¥¼ ì‚¬ìš©í•œë‹¤ë©´ GPU ê°€ì†í™”ë¥¼ ìœ„í•´ ì–´ë–»ê²Œ í•´ì•¼ í–ˆì„ê¹Œ.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda:0'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">" - device : </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">" - cpu tensor : "</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">" - gpu tensor : "</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</code></pre></div></div>

<p>ì´ë ‡ê²Œ í•˜ë©´ gpu ê°€ì†ì´ ê°€ëŠ¥í–ˆë‹¤. (cuda:0ë²ˆ gpu deviceì— tensor í• ë‹¹)
<img src="https://velog.velcdn.com/images/bolero2/post/676b9471-2900-4f88-b2dd-2d399627ce8c/image.png" alt="" /></p>

<p>ì´ë ‡ê²Œ, gpu tensor ì¶œë ¥ ë¶€ë¶„ì˜ <code class="language-plaintext highlighter-rouge">device</code> ìª½ì— <strong><code class="language-plaintext highlighter-rouge">â€˜cuda:0â€™</code></strong>ì´ ì˜ ë“¤ì–´ê°„ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ M1ì€ ì–´ë””ì— í• ë‹¹í•´ì¤˜ì•¼ í• ê¹Œ?</p>

<p>M1 ì˜ ê²½ìš°ì—ëŠ”, â€˜cudaâ€™ ë§ê³  <strong>â€˜mpsâ€™</strong>ì— í• ë‹¹í•´ì•¼ í•œë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'mps'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">" - device : </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">" - cpu tensor : "</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">" - gpu tensor : "</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</code></pre></div></div>

<p>í•´ë‹¹ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´
<img src="https://velog.velcdn.com/images/bolero2/post/7348052f-5f0b-478b-9fa3-b24dd9c9bbdd/image.png" alt="" /></p>

<p><code class="language-plaintext highlighter-rouge">device=</code> ìª½ì— <code class="language-plaintext highlighter-rouge">â€˜cuda:0â€™</code> ì´ ì•„ë‹Œ <strong><code class="language-plaintext highlighter-rouge">â€˜mpsâ€™</code></strong> ê°€ ë“¤ì–´ê°€ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p>

<h1 id="4-speed-comparison">4. Speed Comparison</h1>

<p>ì†ë„ ë¹„êµë¥¼ í•´ë³´ì.</p>

<p>ë‹¹ì—°íˆ nvidia gpuë³´ë‹¤ëŠ” ëŠë¦´ ê²ƒìœ¼ë¡œ ì˜ˆìƒì´ ë˜ê¸´ í•˜ì§€ë§Œâ€¦ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚ ê¹Œ?</p>

<p>ë¹„êµ ëŒ€ìƒì€ ì´ 4ì¢…ë¥˜ì´ë‹¤.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Machine type</th>
      <th style="text-align: left">Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Server GPU</td>
      <td style="text-align: left">Nvidia RTX A6000</td>
    </tr>
    <tr>
      <td style="text-align: left">M1 GPU</td>
      <td style="text-align: left">M1 GPU, 8 Core</td>
    </tr>
    <tr>
      <td style="text-align: left">Server CPU</td>
      <td style="text-align: left">Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz * 40 Core</td>
    </tr>
    <tr>
      <td style="text-align: left">M1 CPU</td>
      <td style="text-align: left">M1 CPU</td>
    </tr>
  </tbody>
</table>

<p>ê°„ë‹¨í•˜ê²Œ MNIST ë°ì´í„°ì…‹ì„ ë¶„ë¥˜í•˜ëŠ” torch ëª¨ë¸ì„ ì‘ì„±í–ˆë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.nn.functional</span> <span class="kn">import</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>

<span class="c1"># device setting, nvidia='cuda:0' | m1='mps' | cpu='cpu'
</span><span class="n">DEVICE</span> <span class="o">=</span> <span class="s">'cuda:0'</span>    <span class="c1"># "cuda:0" or "mps" or "cpu"
</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span> 
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ch</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">ch</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">256</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">actv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">maxpool</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">fn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fn3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">actv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fn3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">train_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span> <span class="o">=</span> <span class="s">'./data/MNIST'</span><span class="p">,</span>
    <span class="n">train</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()</span> <span class="c1"># ë°ì´í„°ë¥¼ 0ì—ì„œ 255ê¹Œì§€ ìˆëŠ” ê°’ì„ 0ì—ì„œ 1ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
</span>    <span class="p">])</span>
<span class="p">)</span>

<span class="n">test_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span>
    <span class="n">root</span> <span class="o">=</span> <span class="s">'./data/MNIST'</span><span class="p">,</span>
    <span class="n">train</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="n">download</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()</span> <span class="c1"># ë°ì´í„°ë¥¼ 0ì—ì„œ 255ê¹Œì§€ ìˆëŠ” ê°’ì„ 0ì—ì„œ 1ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
</span>    <span class="p">])</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">n_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">===== Model Architecture ====="</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">===== Model Parameters ====="</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">" - {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">n_params</span><span class="p">),</span> <span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>

<span class="n">total_train_iter</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_set</span><span class="p">)</span> <span class="o">/</span> <span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">total_valid_iter</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span> <span class="o">/</span> <span class="n">BATCH_SIZE</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">train_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">epochs_times</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n\n</span><span class="s">Training start time : {}</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">()))</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
    <span class="n">epoch_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>

    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">iter_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">image</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span>

        <span class="n">image</span>  <span class="o">=</span> <span class="n">image</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

        <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span> <span class="o">==</span> <span class="n">target</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">/</span> <span class="n">BATCH_SIZE</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">train_acc</span> <span class="o">+=</span> <span class="n">acc</span>

        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="nb">int</span><span class="p">(</span><span class="n">total_train_iter</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"[train %5s/%5s] Epoch: %4s | Time: %6.2fs | loss: %10.4f | Acc: %10.4f"</span> <span class="o">%</span> <span class="p">(</span>
                            <span class="n">step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">total_train_iter</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">iter_start</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">acc</span><span class="p">)))</span>

    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="n">total_train_iter</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">train_acc</span> <span class="o">/</span> <span class="n">total_train_iter</span>
    <span class="n">epoch_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">epoch_start</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"[Epoch {} training Ended] &gt; Time: {:.2}s/epoch | Loss: {:.4f} | Acc: {:g}</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
        <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">epoch_runtime</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_loss</span><span class="p">),</span> <span class="n">train_acc</span><span class="p">))</span>
    
    <span class="n">epochs_times</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">epoch_runtime</span><span class="p">)</span>

<span class="n">program_runtime</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">train_start</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n\n</span><span class="s">Training running time : {:.2}</span><span class="se">\n\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">program_runtime</span><span class="p">))</span>

<span class="n">epochs_times</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">epochs_times</span><span class="p">))</span>
<span class="n">epochs_times</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">epochs_times</span><span class="p">))</span>

<span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">basename</span><span class="p">(</span><span class="n">__file__</span><span class="p">).</span><span class="n">split</span><span class="p">(</span><span class="s">'.'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="s">".txt"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">writelines</span><span class="p">(</span><span class="n">epochs_times</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'save success epoch times! -&gt; </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>ì‹¤í—˜í•˜ë‹¤ë³´ë‹ˆ ì—„ì²­ë‚œ ë­”ê°€ê°€ ìˆì—ˆë‹¤â€¦</p>

<blockquote>
  <p>ğŸ§ ì‹¤í—˜í•˜ë©´ì„œ í¬ìŠ¤íŒ… ì‘ì„± ì¤‘ì¸ë°, m1ì—ì„œ image size=224 ë¡œ í•˜ê³  batch_size=64ë¡œ ì„¤ì • í›„ì— í•™ìŠµì„ ì‹¤í–‰í•˜ë©´ ì“°ë¡œí‹€ë§ì´ ê±¸ë¦°ë‹¤!!! ì´ê²Œ ë§ë‚˜??</p>

  <p>ì•„ì§ ëŒ€ê·œëª¨ì˜ í•™ìŠµì€ ì œëŒ€ë¡œ ì˜ ì•ˆë˜ëŠ” ëŠë‚Œì´ë‹¤â€¦</p>
</blockquote>

<ul>
  <li><strong>1 Epoch ê²½ê³¼ ì‹œê°„ (M1 GPU vs Server GPU)</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>base<span class="o">)</span> bolero is <span class="k">in </span>now ~ <span class="nv">$ </span>python m1_gpu.py 

<span class="o">=====</span> Model Architecture <span class="o">=====</span>
Model<span class="o">(</span>
  <span class="o">(</span>conv1<span class="o">)</span>: Conv2d<span class="o">(</span>1, 128, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn1<span class="o">)</span>: BatchNorm2d<span class="o">(</span>128, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>conv2<span class="o">)</span>: Conv2d<span class="o">(</span>128, 256, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn2<span class="o">)</span>: BatchNorm2d<span class="o">(</span>256, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>actv<span class="o">)</span>: ReLU<span class="o">(</span><span class="nv">inplace</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>maxpool<span class="o">)</span>: MaxPool2d<span class="o">(</span><span class="nv">kernel_size</span><span class="o">=</span>2, <span class="nv">stride</span><span class="o">=</span>2, <span class="nv">padding</span><span class="o">=</span>0, <span class="nv">dilation</span><span class="o">=</span>1, <span class="nv">ceil_mode</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>dropout<span class="o">)</span>: Dropout2d<span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.5, <span class="nv">inplace</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>flatten<span class="o">)</span>: Flatten<span class="o">(</span><span class="nv">start_dim</span><span class="o">=</span>1, <span class="nv">end_dim</span><span class="o">=</span><span class="nt">-1</span><span class="o">)</span>
  <span class="o">(</span>fn1<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12544, <span class="nv">out_features</span><span class="o">=</span>256, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn2<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>256, <span class="nv">out_features</span><span class="o">=</span>100, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn3<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>100, <span class="nv">out_features</span><span class="o">=</span>10, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
<span class="o">)</span> 

<span class="o">=====</span> Model Parameters <span class="o">=====</span>
 - 3535446 

Training start <span class="nb">time</span> : 2022-06-01 12:36:36.501451

/Users/bolero/m1_gpu.py:63: UserWarning: Implicit dimension choice <span class="k">for </span>softmax has been deprecated. Change the call to include <span class="nv">dim</span><span class="o">=</span>X as an argument.
  <span class="k">return </span>softmax<span class="o">(</span>x<span class="o">)</span>
<span class="o">[</span>train     1/ 3750] Epoch:    1 | Time:   0.32s | loss:     2.3119 | Acc:     0.1250
<span class="o">[</span>train  1876/ 3750] Epoch:    1 | Time:   0.03s | loss:     1.4933 | Acc:     0.9375
<span class="o">[</span>Epoch 1 training Ended] <span class="o">&gt;</span> Time: 1.3e+02s/epoch | Loss: 1.5247 | Acc: 0.9397
</code></pre></div></div>

<p><strong><em>M1 Macbook Pro</em></strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>neural<span class="o">)</span> NvidiaServer@NvidiaServer:~<span class="nv">$ </span>python server_gpu.py 

<span class="o">=====</span> Model Architecture <span class="o">=====</span>
Model<span class="o">(</span>
  <span class="o">(</span>conv1<span class="o">)</span>: Conv2d<span class="o">(</span>1, 128, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn1<span class="o">)</span>: BatchNorm2d<span class="o">(</span>128, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>conv2<span class="o">)</span>: Conv2d<span class="o">(</span>128, 256, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn2<span class="o">)</span>: BatchNorm2d<span class="o">(</span>256, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>actv<span class="o">)</span>: ReLU<span class="o">(</span><span class="nv">inplace</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>maxpool<span class="o">)</span>: MaxPool2d<span class="o">(</span><span class="nv">kernel_size</span><span class="o">=</span>2, <span class="nv">stride</span><span class="o">=</span>2, <span class="nv">padding</span><span class="o">=</span>0, <span class="nv">dilation</span><span class="o">=</span>1, <span class="nv">ceil_mode</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>dropout<span class="o">)</span>: Dropout2d<span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.5, <span class="nv">inplace</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>flatten<span class="o">)</span>: Flatten<span class="o">(</span><span class="nv">start_dim</span><span class="o">=</span>1, <span class="nv">end_dim</span><span class="o">=</span><span class="nt">-1</span><span class="o">)</span>
  <span class="o">(</span>fn1<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12544, <span class="nv">out_features</span><span class="o">=</span>256, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn2<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>256, <span class="nv">out_features</span><span class="o">=</span>100, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn3<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>100, <span class="nv">out_features</span><span class="o">=</span>10, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
<span class="o">)</span> 

<span class="o">=====</span> Model Parameters <span class="o">=====</span>
 - 3535446 

Training start <span class="nb">time</span> : 2022-06-01 03:36:49.959508

server_gpu.py:63: UserWarning: Implicit dimension choice <span class="k">for </span>softmax has been deprecated. Change the call to include <span class="nv">dim</span><span class="o">=</span>X as an argument.
  <span class="k">return </span>softmax<span class="o">(</span>x<span class="o">)</span>
<span class="o">[</span>train     1/ 3750] Epoch:    1 | Time:   0.04s | loss:     2.3063 | Acc:     0.0000
<span class="o">[</span>train  1876/ 3750] Epoch:    1 | Time:   0.01s | loss:     1.4613 | Acc:     1.0000
<span class="o">[</span>Epoch 1 training Ended] <span class="o">&gt;</span> Time: 2.7e+01s/epoch | Loss: 1.5264 | Acc: 0.938167
</code></pre></div></div>

<p><strong><em>Nvidia RTX A6000</em></strong></p>

<blockquote>
  <p>ğŸ§ 1 Epoch ê¹Œì§€ë§Œ ë´¤ì„ ë•Œ, Nvidia gpu ì„œë²„ìš©<strong>(RTX A6000)</strong> ì—ì„œëŠ” 27ì´ˆ~28ì´ˆ/epochê°€ ê±¸ë ¸ê³ , <strong><em>m1 ì—ì„œëŠ” 130ì´ˆ~140ì´ˆ/epochê°€ ê±¸ë ¸ë‹¤.</em>
ì•½ 5ë°°ì •ë„ ì°¨ì´ë‚˜ëŠ”ë°</strong>, í˜„ì¬ ì„œë²„ìš© GPU ê°€ ë„ˆë¬´ ìš°ì›”í•œ ì ë„ ìˆë‹¤.</p>
</blockquote>

<ul>
  <li><strong>1 Epoch ê²½ê³¼ ì‹œê°„ (M1 GPU vs Server CPU)</strong></li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>base<span class="o">)</span> bolero is <span class="k">in </span>now ~ <span class="nv">$ </span>python m1_gpu.py 

<span class="o">=====</span> Model Architecture <span class="o">=====</span>
Model<span class="o">(</span>
  <span class="o">(</span>conv1<span class="o">)</span>: Conv2d<span class="o">(</span>1, 128, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn1<span class="o">)</span>: BatchNorm2d<span class="o">(</span>128, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>conv2<span class="o">)</span>: Conv2d<span class="o">(</span>128, 256, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn2<span class="o">)</span>: BatchNorm2d<span class="o">(</span>256, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>actv<span class="o">)</span>: ReLU<span class="o">(</span><span class="nv">inplace</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>maxpool<span class="o">)</span>: MaxPool2d<span class="o">(</span><span class="nv">kernel_size</span><span class="o">=</span>2, <span class="nv">stride</span><span class="o">=</span>2, <span class="nv">padding</span><span class="o">=</span>0, <span class="nv">dilation</span><span class="o">=</span>1, <span class="nv">ceil_mode</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>dropout<span class="o">)</span>: Dropout2d<span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.5, <span class="nv">inplace</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>flatten<span class="o">)</span>: Flatten<span class="o">(</span><span class="nv">start_dim</span><span class="o">=</span>1, <span class="nv">end_dim</span><span class="o">=</span><span class="nt">-1</span><span class="o">)</span>
  <span class="o">(</span>fn1<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12544, <span class="nv">out_features</span><span class="o">=</span>256, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn2<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>256, <span class="nv">out_features</span><span class="o">=</span>100, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn3<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>100, <span class="nv">out_features</span><span class="o">=</span>10, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
<span class="o">)</span> 

<span class="o">=====</span> Model Parameters <span class="o">=====</span>
 - 3535446 

Training start <span class="nb">time</span> : 2022-06-01 12:36:36.501451

/Users/bolero/m1_gpu.py:63: UserWarning: Implicit dimension choice <span class="k">for </span>softmax has been deprecated. Change the call to include <span class="nv">dim</span><span class="o">=</span>X as an argument.
  <span class="k">return </span>softmax<span class="o">(</span>x<span class="o">)</span>
<span class="o">[</span>train     1/ 3750] Epoch:    1 | Time:   0.32s | loss:     2.3119 | Acc:     0.1250
<span class="o">[</span>train  1876/ 3750] Epoch:    1 | Time:   0.03s | loss:     1.4933 | Acc:     0.9375
<span class="o">[</span>Epoch 1 training Ended] <span class="o">&gt;</span> Time: 1.3e+02s/epoch | Loss: 1.5247 | Acc: 0.9397
</code></pre></div></div>

<p><strong><em>M1 Macbook Pro</em></strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>neural<span class="o">)</span> NvidiaServer@NvidiaServer:~<span class="nv">$ </span>python server_cpu.py 

<span class="o">=====</span> Model Architecture <span class="o">=====</span>
Model<span class="o">(</span>
  <span class="o">(</span>conv1<span class="o">)</span>: Conv2d<span class="o">(</span>1, 128, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn1<span class="o">)</span>: BatchNorm2d<span class="o">(</span>128, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>conv2<span class="o">)</span>: Conv2d<span class="o">(</span>128, 256, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn2<span class="o">)</span>: BatchNorm2d<span class="o">(</span>256, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>actv<span class="o">)</span>: ReLU<span class="o">(</span><span class="nv">inplace</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>maxpool<span class="o">)</span>: MaxPool2d<span class="o">(</span><span class="nv">kernel_size</span><span class="o">=</span>2, <span class="nv">stride</span><span class="o">=</span>2, <span class="nv">padding</span><span class="o">=</span>0, <span class="nv">dilation</span><span class="o">=</span>1, <span class="nv">ceil_mode</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>dropout<span class="o">)</span>: Dropout2d<span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.5, <span class="nv">inplace</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>flatten<span class="o">)</span>: Flatten<span class="o">(</span><span class="nv">start_dim</span><span class="o">=</span>1, <span class="nv">end_dim</span><span class="o">=</span><span class="nt">-1</span><span class="o">)</span>
  <span class="o">(</span>fn1<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12544, <span class="nv">out_features</span><span class="o">=</span>256, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn2<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>256, <span class="nv">out_features</span><span class="o">=</span>100, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn3<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>100, <span class="nv">out_features</span><span class="o">=</span>10, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
<span class="o">)</span> 

<span class="o">=====</span> Model Parameters <span class="o">=====</span>
 - 3535446 

Training start <span class="nb">time</span> : 2022-06-01 03:37:22.395249

server_cpu.py:63: UserWarning: Implicit dimension choice <span class="k">for </span>softmax has been deprecated. Change the call to include <span class="nv">dim</span><span class="o">=</span>X as an argument.
  <span class="k">return </span>softmax<span class="o">(</span>x<span class="o">)</span>
<span class="o">[</span>train     1/ 3750] Epoch:    1 | Time:   1.38s | loss:     2.3021 | Acc:     0.0625
<span class="o">[</span>train  1876/ 3750] Epoch:    1 | Time:   0.04s | loss:     1.4617 | Acc:     1.0000
<span class="o">[</span>Epoch 1 training Ended] <span class="o">&gt;</span> Time: 1.8e+02s/epoch | Loss: 1.5256 | Acc: 0.939617
</code></pre></div></div>

<p><strong><em>Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz * 40 Core</em></strong></p>

<blockquote>
  <p>ğŸ§ ë‹¤í–‰ìŠ¤ëŸ½ê²Œë„(?) <strong>M1 GPUê°€ Serverì˜ 40 Core CPUë³´ë‹¤ëŠ” ë¹¨ëë‹¤.</strong>
ìˆ˜ì¹˜ë¡œ ë³´ìë©´ M1 GPUê°€ 130ì´ˆ/epoch ì •ë„ ë‚˜ì˜¤ê³ , 40 Core CPUê°€ 180ì´ˆ/epoch ì •ë„ ë‚˜ì˜¨ë‹¤. (<strong>ì•½ 1.4ë°°ì •ë„ M1 GPUê°€ ë” ë¹ ë¦„</strong>)</p>
</blockquote>

<ol>
  <li><strong>1 Epoch ê²½ê³¼ ì‹œê°„ (M1 CPU vs M1 GPU)</strong></li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>base<span class="o">)</span> bolero is <span class="k">in </span>now ~ <span class="nv">$ </span>python m1_cpu.py 

<span class="o">=====</span> Model Architecture <span class="o">=====</span>
Model<span class="o">(</span>
  <span class="o">(</span>conv1<span class="o">)</span>: Conv2d<span class="o">(</span>1, 128, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn1<span class="o">)</span>: BatchNorm2d<span class="o">(</span>128, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>conv2<span class="o">)</span>: Conv2d<span class="o">(</span>128, 256, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn2<span class="o">)</span>: BatchNorm2d<span class="o">(</span>256, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>actv<span class="o">)</span>: ReLU<span class="o">(</span><span class="nv">inplace</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>maxpool<span class="o">)</span>: MaxPool2d<span class="o">(</span><span class="nv">kernel_size</span><span class="o">=</span>2, <span class="nv">stride</span><span class="o">=</span>2, <span class="nv">padding</span><span class="o">=</span>0, <span class="nv">dilation</span><span class="o">=</span>1, <span class="nv">ceil_mode</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>dropout<span class="o">)</span>: Dropout2d<span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.5, <span class="nv">inplace</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>flatten<span class="o">)</span>: Flatten<span class="o">(</span><span class="nv">start_dim</span><span class="o">=</span>1, <span class="nv">end_dim</span><span class="o">=</span><span class="nt">-1</span><span class="o">)</span>
  <span class="o">(</span>fn1<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12544, <span class="nv">out_features</span><span class="o">=</span>256, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn2<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>256, <span class="nv">out_features</span><span class="o">=</span>100, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn3<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>100, <span class="nv">out_features</span><span class="o">=</span>10, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
<span class="o">)</span> 

<span class="o">=====</span> Model Parameters <span class="o">=====</span>
 - 3535446 

Training start <span class="nb">time</span> : 2022-06-01 12:36:37.141268

/Users/bolero/m1_cpu.py:63: UserWarning: Implicit dimension choice <span class="k">for </span>softmax has been deprecated. Change the call to include <span class="nv">dim</span><span class="o">=</span>X as an argument.
  <span class="k">return </span>softmax<span class="o">(</span>x<span class="o">)</span>
<span class="o">[</span>train     1/ 3750] Epoch:    1 | Time:   0.09s | loss:     2.3023 | Acc:     0.1250
<span class="o">[</span>train  1876/ 3750] Epoch:    1 | Time:   0.05s | loss:     1.4612 | Acc:     1.0000
<span class="o">[</span>Epoch 1 training Ended] <span class="o">&gt;</span> Time: 1.7e+02s/epoch | Loss: 1.5262 | Acc: 0.938867
</code></pre></div></div>

<p><strong><em>M1 Macbook Pro - CPU</em></strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">(</span>base<span class="o">)</span> bolero is <span class="k">in </span>now ~ <span class="nv">$ </span>python m1_gpu.py 

<span class="o">=====</span> Model Architecture <span class="o">=====</span>
Model<span class="o">(</span>
  <span class="o">(</span>conv1<span class="o">)</span>: Conv2d<span class="o">(</span>1, 128, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn1<span class="o">)</span>: BatchNorm2d<span class="o">(</span>128, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>conv2<span class="o">)</span>: Conv2d<span class="o">(</span>128, 256, <span class="nv">kernel_size</span><span class="o">=(</span>3, 3<span class="o">)</span>, <span class="nv">stride</span><span class="o">=(</span>1, 1<span class="o">)</span>, <span class="nv">padding</span><span class="o">=(</span>1, 1<span class="o">))</span>
  <span class="o">(</span>bn2<span class="o">)</span>: BatchNorm2d<span class="o">(</span>256, <span class="nv">eps</span><span class="o">=</span>1e-05, <span class="nv">momentum</span><span class="o">=</span>0.1, <span class="nv">affine</span><span class="o">=</span>True, <span class="nv">track_running_stats</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>actv<span class="o">)</span>: ReLU<span class="o">(</span><span class="nv">inplace</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>maxpool<span class="o">)</span>: MaxPool2d<span class="o">(</span><span class="nv">kernel_size</span><span class="o">=</span>2, <span class="nv">stride</span><span class="o">=</span>2, <span class="nv">padding</span><span class="o">=</span>0, <span class="nv">dilation</span><span class="o">=</span>1, <span class="nv">ceil_mode</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>dropout<span class="o">)</span>: Dropout2d<span class="o">(</span><span class="nv">p</span><span class="o">=</span>0.5, <span class="nv">inplace</span><span class="o">=</span>False<span class="o">)</span>
  <span class="o">(</span>flatten<span class="o">)</span>: Flatten<span class="o">(</span><span class="nv">start_dim</span><span class="o">=</span>1, <span class="nv">end_dim</span><span class="o">=</span><span class="nt">-1</span><span class="o">)</span>
  <span class="o">(</span>fn1<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>12544, <span class="nv">out_features</span><span class="o">=</span>256, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn2<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>256, <span class="nv">out_features</span><span class="o">=</span>100, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
  <span class="o">(</span>fn3<span class="o">)</span>: Linear<span class="o">(</span><span class="nv">in_features</span><span class="o">=</span>100, <span class="nv">out_features</span><span class="o">=</span>10, <span class="nv">bias</span><span class="o">=</span>True<span class="o">)</span>
<span class="o">)</span> 

<span class="o">=====</span> Model Parameters <span class="o">=====</span>
 - 3535446 

Training start <span class="nb">time</span> : 2022-06-01 12:36:36.501451

/Users/bolero/m1_gpu.py:63: UserWarning: Implicit dimension choice <span class="k">for </span>softmax has been deprecated. Change the call to include <span class="nv">dim</span><span class="o">=</span>X as an argument.
  <span class="k">return </span>softmax<span class="o">(</span>x<span class="o">)</span>
<span class="o">[</span>train     1/ 3750] Epoch:    1 | Time:   0.32s | loss:     2.3119 | Acc:     0.1250
<span class="o">[</span>train  1876/ 3750] Epoch:    1 | Time:   0.03s | loss:     1.4933 | Acc:     0.9375
<span class="o">[</span>Epoch 1 training Ended] <span class="o">&gt;</span> Time: 1.3e+02s/epoch | Loss: 1.5247 | Acc: 0.9397
</code></pre></div></div>

<p><strong><em>M1 Macbook Pro - GPU</em></strong></p>

<blockquote>
  <p>ğŸ§ ì†”ì§íˆ M1-CPU ë‚˜ M1-GPU ë‚˜ ë­”ê°€ ì½”ë”© ì˜ëª»í–ˆì„ ì¤„ ì•Œê³  ê¸°ëŒ€ ì•ˆí–ˆëŠ”ë°, ì˜ì™¸ë¡œ ì°¨ì´ê°€ ë°œìƒí–ˆë‹¤.
<strong>M1-CPUê°€ 170ì´ˆ~180ì´ˆ/epoch ì •ë„ ë‚˜ì˜¤ê³ , M1-GPUëŠ” 130ì´ˆ/epoch ì •ë„ ë‚˜ì˜¤ëŠ”ë°, M1-CPU ì˜ ìˆ˜ì¤€ì´ Intel Xeon 4210R 2.40GHz 40 Core ì •ë„ì˜ ìˆ˜ì¤€ì„ ë³´ì˜€ë‹¤.</strong></p>
</blockquote>

<p>ê²°ë¡ ì€â€¦ M1-GPU ëŠ” ì—„ì²­ ì“¸ ì •ë„ëŠ” ì•„ë‹ˆë‹¤ ì•„ì§! ê·¸ë˜ë„ CPUë³´ë‹¤ëŠ” ë‚˜ìœ¼ë‹ˆ ê°„ë‹¨í•œ í…ŒìŠ¤íŒ… ì •ë„ëŠ” ë¬´ë¦¬ì—†ì´ ëŒë¦´ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë¨.</p>

<p><strong>(ë‹¨ì ì€, ì•„ê¹Œ ì‹¤í—˜í•´ë´¤ëŠ”ë° <code class="language-plaintext highlighter-rouge">image_size=[224, 224]</code> ë¡œ í•˜ê³  <code class="language-plaintext highlighter-rouge">batch_size=64</code> ë¡œ í•˜ë‹ˆê¹Œ macbookì— 5~7ì´ˆë§ˆë‹¤ ì“°ë¡œí‹€ë§ì´ ë°œìƒí–ˆë‹¤â€¦ğŸ¥²)</strong></p>

<p>í•™ìŠµ ì¢…ë£Œ ë•Œ ì‹¤í—˜ ë°©ì‹ ë³„ ë§¤ Epochì˜ ì†Œìš”ì‹œê°„ì„ ì €ì¥í•˜ëŠ”ë°, ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ëŠ” ê±¸ë¡œ í¬ìŠ¤íŒ…ì„ ë§ˆë¬´ë¦¬ í•˜ê² ë‹¤.
(server CPUëŠ” ì„œë²„ ë¨¸ì‹ ì˜ ë‹¤ë¥¸ ì‘ì—…ë•Œë¬¸ì— 20 Epochsê¹Œì§€ ì¸¡ì •í•˜ì§€ ëª»í•¨.)</p>

<p><strong>ì†ŒìŠ¤ì½”ë“œ</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">server_gpu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)]</span>
<span class="n">m1_gpu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)]</span>
<span class="n">m1_cpu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)]</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"server_gpu.txt"</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">server_gpu</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">server_gpu</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">server_gpu</span><span class="p">))</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"m1_gpu.txt"</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">m1_gpu</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">m1_gpu</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">m1_gpu</span><span class="p">))</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"m1_cpu.txt"</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">m1_cpu</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">m1_cpu</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="n">m1_cpu</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training time per devices'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">server_gpu</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'green'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">m1_gpu</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">m1_cpu</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'Server GPU'</span><span class="p">,</span> <span class="s">'Server CPU'</span><span class="p">,</span> <span class="s">'M1 GPU'</span><span class="p">,</span> <span class="s">'M1 CPU'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Epochs"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Running time(seconds)"</span><span class="p">)</span>
<span class="c1"># plt.show()
</span><span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"runtime.png"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>ê²°ê³¼ ì´ë¯¸ì§€</strong><br />
<img src="https://velog.velcdn.com/images/bolero2/post/b04b61e8-e95a-46be-b214-049e490f2955/image.png" alt="" /></p>

:ET